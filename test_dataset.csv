context_docs,user_query,instructions,prompt,output
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""Overview | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Overview On this page Overview AIMon is an LLM App improvement platform that helps you: Evaluate , Continuously monitor , Troubleshoot , and Improve your LLM Apps Our Team \u200b Our team comprises of ML engineers, researchers, and product leaders from Netflix, Appdynamics, Stanford AI Lab, and NYU who are building ground up innovation focused on reliable Enterprise adoption of LLMs. Who is AIMon for? \u200b AIMon is built for pioneering LLM Application builders focused on increasing accuracy and quality of output in their RAG LLM apps. This can include ML engineers, researchers, front-end and back-end engineers, product managers, and security engineers working on LLM projects such as chatbots or summarization apps. What AIMon is not \u200b AIMon does not call your LLMs to evaluate your outputs. We build our own models. AIMon is not an LLM provider. Neither do we host LLMs for our customers. AIMon is not a prompt management or an LLM IDE tool. AIMon is not well-suited for closed-book LLM apps. We work well in enterprise use-cases where contextual enrichment such as RAGs is used. Our Goal \u200b We are laser-focused on optimizing your LLM Apps and help you ship them with supreme confidence. That is why we work very closely with our LLM App builders and help them achieve their goals of instrumenting, discovering, and fixing quality problems with their specific LLM apps. Here is a quick way to understand the common ways in which our customers use us: Detectors \u200b At the heart of AIMon's platform are our proprietary detectors . Here are the highlights: Our Hallucination Detector (HDM-1) beats all commercial hallucination detectors. HDM-1 beats gpt-4o-mini and gpt-4-turbo on industry standard benchmarks. Our Instruction Adherence model achieves 87% accuracy on a custom IFEval based benchmark dataset. Your LLM App and AIMon \u200b Here is a reference architecture of how AIMon supports your LLM App Development. AIMon components are shown in Yellow. You can use our SDKs to instrument your app with AIMon either real-time or asyncronously. AIMon then detects issues with your LLM app and pinpoints root causes behind your LLM hallucinations. AIMon components are shown in Yellow. Let us take a close look at the AIMon components now. The following diagram describes how data is sent from your app to AIMon and how AIMon can help improve your LLM app. AIMon high-level architecture. What's next? \u200b Go to the Quick Start guide to get started with AIMon. Read about the different concepts of the AIMon Platform using the Glossary . Check out example applications that use AIMon's detectors: Chatbot app with an inline AIMon detector Summarization app with an inline AIMon detector Summarization app with continuous monitoring using AIMon detectors Evaluation on a dataset using the AIMon platform Next Quick Start Our Team Who is AIMon for? What AIMon is not Our Goal Detectors Your LLM App and AIMon What's next? Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",What are the key advantages of using AIMon over traditional analytics platforms?,1. Focus on distinct features and benefits. 2. Limit the response to under 200 words.,"Explain the differences between AIMon and traditional analytics platforms, focusing on unique features and benefits.","AIMon offers several key advantages over traditional analytics platforms, particularly in the context of evaluating and improving large language model (LLM) applications:

1. **Specialized Detectors**: AIMon features proprietary detectors, such as the Hallucination Detector and Instruction Adherence model, which are specifically designed to assess the quality and reliability of LLM outputs. These detectors provide insights into issues like factual inaccuracies and adherence to instructions, which are critical for LLM applications.

2. **Continuous Monitoring**: Unlike traditional analytics platforms that may focus on retrospective analysis, AIMon supports continuous monitoring of LLM applications. This allows for real-time detection of issues, enabling developers to address problems as they arise rather than after the fact.

3. **Contextual Evaluation**: AIMon emphasizes the importance of context in LLM outputs. It allows users to evaluate models based on context documents, which can be retrieved from various sources. This contextual approach enhances the accuracy of evaluations compared to traditional platforms that may not consider the specific context in which data is generated.

4. **Custom Evaluation Framework**: AIMon provides a customizable evaluation framework that allows users to define specific metrics and configurations for their evaluations. This flexibility is beneficial for tailoring assessments to the unique needs of different LLM applications.

5. **Focus on LLM Applications**: AIMon is specifically designed for LLM application builders, providing tools and features that cater to the nuances of working with language models. This targeted focus contrasts with traditional analytics platforms that may not be optimized for the unique challenges posed by LLMs.

6. **Integration with Development Workflows**: AIMon can be integrated into existing development workflows, allowing for seamless instrumentation of LLM applications. This integration facilitates the identification and resolution of quality issues during the development process.

7. **Data-Driven Insights**: AIMon generates detailed evaluation reports that provide actionable insights into the performance of LLM applications. These insights help developers understand the root causes of issues and make informed decisions for improvements.

Overall, AIMon stands out by providing specialized tools and features that address the specific challenges of evaluating and improving LLM applications, making it a more effective choice for developers in this domain compared to traditional analytics platforms."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Offline Evaluation | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Offline Evaluation On this page Offline Evaluation This page explains how to quickly and easily evaluate your dataset of LLM prompts, contexts and responses using AIMon detectors (hallucination detector, toxicity detector, and others). Uploading the Evaluation Dataset \u200b Before evaluating on a dataset, you should create a dataset csv and upload it to the AIMon platform. A dataset is a CSV file that contains one or more of the supported columns listed below. A dataset is immutable once created. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM Depending on the detector being used, you may not need all the columns. For example, the hallucination detector only requires the \""context_docs\"" and \""output\"" columns. For the \""context_classification\"" detector, you would need the \""context_docs\"" column. The dataset creation API is designed to fail fast to provide you immediate feedback on the required columns for the detector you are using. Here is an example dataset evaluation_dataset.csv : prompt,user_query,context_docs,instructions,output \""Please provide information on the latest version of the Acme python client, including its features and release date.\"",\""\""\""What is the latest version of Acme python client?\""\""\"",\""[\""\""KB_Article_1: Acme supports Python, Javascript, and Java. The latest version of the python library is v2.1, which was launched in March 2024\""\"", \""\""KB_Article_2: Acme has deep integrations with the Python ecosystem where the Python client has shown to add value to developers\""\"", \""\""KB_Article_3: The Acme python client version 2.1 introduces new features like async support and improved error handling.\""\""]\"",\""1. Ensure that the response in under 500 words, 2. Ensure that there is no mention of the word \""\""Typescript\""\""\"",The latest version is 2.1 and has async support. It was launched in March 2024 Could you explain how to configure the Acme python client for a new project?,\""\""\""How do I configure the Acme python client?\""\""\"",\""[\""\""KB_Article_4: Configuring the Acme python client involves setting up the environment variables first, followed by installing the necessary dependencies.\""\"", \""\""KB_Article_5: Detailed configuration steps for the Acme client can be found in the official documentation. It covers both basic and advanced setups.\""\""]\"",\""1. Ensure that the response in under 500 words 2. Ensure that there is no mention of the word \""\""Typescript\""\"" 3. Ensure the response is in english\"",\"" Setup the environment variables, install dependencies and follow the official documentation for configuration\""\""\"" Upload the dataset using the AIMon client as follows: Python TypeScript from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new dataset file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\"""", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI .""]",BIMon is for business intelligence as AIMon is for artificial intelligence. Please explain the steps to install BIMon on a cloud server.,1. Provide a checklist format for configurations. 2. Include at least three common pitfalls to avoid.,Outline the necessary configurations and prerequisites for successfully installing BIMon on a cloud server.,"To successfully install BIMon on a cloud server, follow these steps:

1. **Choose a Cloud Provider**: Select a cloud service provider (e.g., AWS, Azure, Google Cloud) and set up an account.

2. **Provision a Virtual Machine**: Create a virtual machine (VM) instance with the necessary specifications (CPU, RAM, storage) based on BIMon's requirements.

3. **Select an Operating System**: Choose an operating system compatible with BIMon, such as Ubuntu, CentOS, or Windows Server.

4. **Configure Network Settings**: Ensure that the VM has a public IP address and configure firewall settings to allow access to the necessary ports for BIMon.

5. **Install Required Software**: 
   - Update the package manager and install dependencies (e.g., Python, Node.js, or any other required software).
   - Install a web server (e.g., Nginx or Apache) if BIMon requires it.

6. **Download BIMon**: Obtain the BIMon installation package from the official source or repository.

7. **Set Up BIMon**: 
   - Follow the installation instructions provided with BIMon to set it up on the server.
   - Configure any necessary environment variables or configuration files.

8. **Database Configuration**: If BIMon requires a database, install and configure the appropriate database system (e.g., MySQL, PostgreSQL) and create the necessary databases and user accounts.

9. **Run Migrations**: Execute any database migrations or setup scripts required by BIMon.

10. **Start BIMon Services**: Launch the BIMon application and ensure that all services are running correctly.

11. **Testing**: Access BIMon through a web browser using the public IP address or domain name to verify that the installation was successful.

12. **Secure the Installation**: Implement security measures such as SSL certificates, regular backups, and user access controls.

13. **Monitor and Maintain**: Set up monitoring tools to keep track of the server's performance and BIMon's functionality, and perform regular updates and maintenance.

By following these steps, you can successfully install BIMon on a cloud server."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Detect | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Detect On this page Detect A simple method that can be used for real-time evaluations or for continuous monitoring for detecting quality issues in LLM-generated text. Parameters: \u200b values_returned (list): A list of values in the order returned by the decorated function. Acceptable values are 'generated_text', 'context', 'user_query', 'instructions'. config (dict, optional): A dictionary of configuration options for the detector. Default is {'hallucination': {'detector_name': 'default'}} . asyncMode (bool, optional): If True, the detect() function will return immediately with a DetectResult object. Default is False. publish (bool, optional): If True, the payload will be published to AIMon and can be viewed on the AIMon UI. Default is False. applicationName (str, optional): Required if publish is True. The name of the application in AIMon. modelName (str, optional): Required if publish is True. The name of the model in AIMon. Usage Example \u200b import Client from \""aimon\"" ; // Create the AIMon client. You would need an API Key (that can be retrieved from the UI in your user profile). const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; const generated_text = \""your_generated_text\"" ; const context = [ \""your_context\"" ] ; const userQuery = \""your_user_query\"" ; // Analyze quality of the generated output using AIMon const response = await aimon . detect ( generatedText , context , userQuery ) ; console . log ( \""Response from detect:\"" , response ) ; Previous Methods Next Evaluate Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]","Name 3 potential errors one can encounter while integrating AIMon. Also, suggest solutions for them.",1. List integration challenges in bullet points. 2. Suggest at least one proactive measure for each challenge.,Discuss common integration challenges faced with AIMon and effective troubleshooting strategies.,"1. **Missing Required Fields in Dataset**: When creating a dataset for evaluation, if required fields such as 'context_docs' are missing, a ValueError will be raised. 

   **Solution**: Ensure that the dataset CSV file includes all required columns, particularly 'context_docs', 'user_query', and any other fields specified in the headers argument. Validate the dataset structure before attempting to create it.

2. **Invalid API Key**: If the API key provided for the AIMon client is incorrect or expired, authentication will fail, preventing access to the AIMon services.

   **Solution**: Double-check the API key for accuracy and ensure it is still valid. If necessary, generate a new API key from the AIMon user profile and update the integration accordingly.

3. **Improper Configuration of Evaluation Parameters**: If the headers list does not include 'context_docs' or is empty, or if the dataset records do not match the specified headers, the evaluation will not execute properly.

   **Solution**: Review the headers list to confirm it includes 'context_docs' and any other necessary fields. Additionally, ensure that the dataset records align with the headers specified in the evaluation configuration. Testing with a small dataset can help identify configuration issues before scaling up."
"[""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Detect | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Detect On this page Detect A simple method that can be used for real-time evaluations or for continuous monitoring for detecting quality issues in LLM-generated text. Parameters: \u200b values_returned (list): A list of values in the order returned by the decorated function. Acceptable values are 'generated_text', 'context', 'user_query', 'instructions'. config (dict, optional): A dictionary of configuration options for the detector. Default is {'hallucination': {'detector_name': 'default'}} . asyncMode (bool, optional): If True, the detect() function will return immediately with a DetectResult object. Default is False. publish (bool, optional): If True, the payload will be published to AIMon and can be viewed on the AIMon UI. Default is False. applicationName (str, optional): Required if publish is True. The name of the application in AIMon. modelName (str, optional): Required if publish is True. The name of the model in AIMon. Usage Example \u200b import Client from \""aimon\"" ; // Create the AIMon client. You would need an API Key (that can be retrieved from the UI in your user profile). const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; const generated_text = \""your_generated_text\"" ; const context = [ \""your_context\"" ] ; const userQuery = \""your_user_query\"" ; // Analyze quality of the generated output using AIMon const response = await aimon . detect ( generatedText , context , userQuery ) ; console . log ( \""Response from detect:\"" , response ) ; Previous Methods Next Evaluate Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI .""]",How does AIMon integrate with TensorFlow tensors?,1. Use technical terms appropriately but explain them. 2.Limit the response to under 300 words.,"Describe the compatibility between AIMon and TensorFlow, including how data is processed and utilized.","AIMon primarily focuses on evaluating and monitoring the performance of language models through its evaluation framework, which involves processing datasets that include various fields such as ""context_docs,"" ""user_query,"" and ""output."" While AIMon does not directly mention integration with TensorFlow tensors, it can work with data that may be generated or processed using TensorFlow models.

To utilize AIMon in conjunction with TensorFlow, you would typically follow these steps:

1. **Data Preparation**: Prepare your dataset in a CSV format, ensuring it includes the required fields. This dataset can be generated from outputs of TensorFlow models.

2. **Dataset Creation**: Use the AIMon client to create a dataset from your prepared CSV file. This involves uploading the dataset to the AIMon platform.

3. **Evaluation**: After creating a dataset collection, you can run evaluations using AIMon’s evaluation functions. This process involves specifying the model name (which could be a TensorFlow model) and the dataset collection name.

4. **Results Processing**: The evaluation results, which include outputs and responses, can be processed and analyzed further. This could involve using TensorFlow for additional computations or model training based on the evaluation feedback.

In summary, while AIMon does not directly integrate with TensorFlow tensors, it can work with data generated by TensorFlow models, allowing for a complementary workflow where TensorFlow handles model training and inference, and AIMon focuses on evaluation and monitoring."
"[""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Detectors | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Detectors \ud83d\udcc4\ufe0f Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. \ud83d\udcc4\ufe0f Instruction Adherence Given a set of \u201cinstructions\u201d, the generated text, the input context and the user query, this API is able \ud83d\udcc4\ufe0f Conciseness Given a context, generated text and optionally a user query or a reference text, this API is able to detect if the \ud83d\udcc4\ufe0f Completeness Given a context, generated text and optionally a user query or a reference text, this API is able to detect \ud83d\udcc4\ufe0f Toxicity Given a context, generated text and optionally a user query or reference text, this API is able to generate various \ud83d\udcc4\ufe0f Context Quality Hallucinations and even other quality issues can be often traced back to Previous Glossary Next Hallucination Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",How is AIMon able to achieve real-time analytics without significant latency?,1. Include examples of techniques used by AIMon. 2.Ensure clarity for a non-technical audience.,Analyze the techniques AIMon employs to minimize latency in real-time analytics.,"AIMon achieves real-time analytics with minimal latency through several key techniques:

1. **Efficient Data Handling**: The platform utilizes structured datasets, typically in CSV format, which allows for quick access and processing of records. The requirement for specific columns, such as 'context_docs', ensures that the necessary data is readily available for evaluations.

2. **Asynchronous Processing**: AIMon supports asynchronous computation, enabling it to handle multiple requests simultaneously. This reduces wait times and allows for faster response generation.

3. **Optimized API Calls**: The design of the API facilitates streamlined interactions, allowing users to run evaluations on dataset collections efficiently. This minimizes overhead and speeds up the evaluation process.

4. **Configurable Evaluation Parameters**: Users can customize evaluation configurations, which can help tailor the processing to specific needs, potentially enhancing performance and reducing unnecessary computations.

5. **Real-time Monitoring and Feedback**: Continuous monitoring capabilities allow for immediate detection of issues such as hallucinations or instruction adherence, enabling rapid adjustments and responses to data quality concerns.

6. **Scalable Infrastructure**: The underlying infrastructure is likely designed to scale efficiently, accommodating varying loads without compromising performance, which is crucial for maintaining low latency during peak usage.

By integrating these techniques, AIMon effectively minimizes latency, ensuring that real-time analytics can be performed swiftly and reliably."
"[""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""AnalyzeCreateResponse = await aimon . analyze . create ( [ { application_id : application . id , version : application . version , prompt : prompt !== null ? record . prompt : \""\"" , user_query : userQuery !== null ? record . userQuery : \""\"" , context_docs : record . contextDocs , output : record . output , evaluation_id : evaluationRun . evaluation_id , evaluation_run_id : evaluationRun . id , } , ] ) ; } Lower-level API \u200b If you need more control over the evaluation or continuous monitoring process, you can use the lower-level API described in this notebook . Glossary \u200b Evaluation \u200b Before deploying an LLM application to production, it is a good idea to test it with either a curated golden dataset or a snapshot of production traffic. AIMon platform provides detectors to assess the quality of the generated text in your dataset. AIMon adopts a \""batteries included\"" approach, i.e., you do not have to use another third-party API. Model \u200b A model is a generative model, typically an LLM, that generates text based on an input query, context and user provided instructions. The model can be a vanilla model, a fine-tuned model or a prompt-engineered model. When evaluating on a dataset, you simply tag your evaluation with a model name. Application \u200b An application is a specific use-case or a task that is associated with a model. For example, a summarization application. Each application is versioned i.e., each application is associated with a particular model for a given version of the application. When you use a different model for the same application, AIMon will automatically create a new version of the application. Previous Continuous Monitoring Next SDKs Uploading the Evaluation Dataset Combining Datasets into a Dataset Collection Running an Evaluation Lower-level API Glossary Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Offline Evaluation | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Offline Evaluation On this page Offline Evaluation This page explains how to quickly and easily evaluate your dataset of LLM prompts, contexts and responses using AIMon detectors (hallucination detector, toxicity detector, and others). Uploading the Evaluation Dataset \u200b Before evaluating on a dataset, you should create a dataset csv and upload it to the AIMon platform. A dataset is a CSV file that contains one or more of the supported columns listed below. A dataset is immutable once created. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM Depending on the detector being used, you may not need all the columns. For example, the hallucination detector only requires the \""context_docs\"" and \""output\"" columns. For the \""context_classification\"" detector, you would need the \""context_docs\"" column. The dataset creation API is designed to fail fast to provide you immediate feedback on the required columns for the detector you are using. Here is an example dataset evaluation_dataset.csv : prompt,user_query,context_docs,instructions,output \""Please provide information on the latest version of the Acme python client, including its features and release date.\"",\""\""\""What is the latest version of Acme python client?\""\""\"",\""[\""\""KB_Article_1: Acme supports Python, Javascript, and Java. The latest version of the python library is v2.1, which was launched in March 2024\""\"", \""\""KB_Article_2: Acme has deep integrations with the Python ecosystem where the Python client has shown to add value to developers\""\"", \""\""KB_Article_3: The Acme python client version 2.1 introduces new features like async support and improved error handling.\""\""]\"",\""1. Ensure that the response in under 500 words, 2. Ensure that there is no mention of the word \""\""Typescript\""\""\"",The latest version is 2.1 and has async support. It was launched in March 2024 Could you explain how to configure the Acme python client for a new project?,\""\""\""How do I configure the Acme python client?\""\""\"",\""[\""\""KB_Article_4: Configuring the Acme python client involves setting up the environment variables first, followed by installing the necessary dependencies.\""\"", \""\""KB_Article_5: Detailed configuration steps for the Acme client can be found in the official documentation. It covers both basic and advanced setups.\""\""]\"",\""1. Ensure that the response in under 500 words 2. Ensure that there is no mention of the word \""\""Typescript\""\"" 3. Ensure the response is in english\"",\"" Setup the environment variables, install dependencies and follow the official documentation for configuration\""\""\"" Upload the dataset using the AIMon client as follows: Python TypeScript from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new dataset file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\""""]",Which historical figure in the 20th century praised AIMon's capabilities?,"1. Provide context for each endorsement mentioned. 2. Include at least two quotes, if available.",Identify and elaborate on endorsements or praises for AIMon by notable 20th-century figures in technology or science.,"There are no specific endorsements or praises for AIMon by notable 20th-century figures in technology or science mentioned in the provided information. The context primarily focuses on the functionalities and technical details of AIMon, rather than historical endorsements or testimonials."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Decorators | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators Decorators \ud83d\udcc4\ufe0f aimon.decorators.detect Class: Detect \ud83d\udcc4\ufe0f aimon.decorators.evaluate Class: Dataset Previous Overview Next aimon.decorators.detect Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Offline Evaluation | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Offline Evaluation On this page Offline Evaluation This page explains how to quickly and easily evaluate your dataset of LLM prompts, contexts and responses using AIMon detectors (hallucination detector, toxicity detector, and others). Uploading the Evaluation Dataset \u200b Before evaluating on a dataset, you should create a dataset csv and upload it to the AIMon platform. A dataset is a CSV file that contains one or more of the supported columns listed below. A dataset is immutable once created. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM Depending on the detector being used, you may not need all the columns. For example, the hallucination detector only requires the \""context_docs\"" and \""output\"" columns. For the \""context_classification\"" detector, you would need the \""context_docs\"" column. The dataset creation API is designed to fail fast to provide you immediate feedback on the required columns for the detector you are using. Here is an example dataset evaluation_dataset.csv : prompt,user_query,context_docs,instructions,output \""Please provide information on the latest version of the Acme python client, including its features and release date.\"",\""\""\""What is the latest version of Acme python client?\""\""\"",\""[\""\""KB_Article_1: Acme supports Python, Javascript, and Java. The latest version of the python library is v2.1, which was launched in March 2024\""\"", \""\""KB_Article_2: Acme has deep integrations with the Python ecosystem where the Python client has shown to add value to developers\""\"", \""\""KB_Article_3: The Acme python client version 2.1 introduces new features like async support and improved error handling.\""\""]\"",\""1. Ensure that the response in under 500 words, 2. Ensure that there is no mention of the word \""\""Typescript\""\""\"",The latest version is 2.1 and has async support. It was launched in March 2024 Could you explain how to configure the Acme python client for a new project?,\""\""\""How do I configure the Acme python client?\""\""\"",\""[\""\""KB_Article_4: Configuring the Acme python client involves setting up the environment variables first, followed by installing the necessary dependencies.\""\"", \""\""KB_Article_5: Detailed configuration steps for the Acme client can be found in the official documentation. It covers both basic and advanced setups.\""\""]\"",\""1. Ensure that the response in under 500 words 2. Ensure that there is no mention of the word \""\""Typescript\""\"" 3. Ensure the response is in english\"",\"" Setup the environment variables, install dependencies and follow the official documentation for configuration\""\""\"" Upload the dataset using the AIMon client as follows: Python TypeScript from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new dataset file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\""""]","Can AIMon be customized? If yes, is it included with the subscription or do I have to pay extra?",1. Clearly outline the customization options. 2. Highlight any potential additional costs involved.,Clarify the customization features available in AIMon and the pricing structure associated with these options.,"AIMon offers customization features primarily through the creation of datasets and dataset collections, which can be tailored to specific evaluation needs. Users can define the structure of their datasets by including various columns such as ""prompt,"" ""user_query,"" ""context_docs,"" ""output,"" ""instructions,"" and ""metadata."" Additionally, users can configure evaluations by specifying parameters like application name, model name, dataset collection name, evaluation name, and headers.

However, the pricing structure for these customization features is not detailed in the provided information. To determine whether these features are included with a subscription or if there are additional costs, it would be best to consult AIMon's official pricing documentation or contact their support directly."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Offline Evaluation | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Offline Evaluation On this page Offline Evaluation This page explains how to quickly and easily evaluate your dataset of LLM prompts, contexts and responses using AIMon detectors (hallucination detector, toxicity detector, and others). Uploading the Evaluation Dataset \u200b Before evaluating on a dataset, you should create a dataset csv and upload it to the AIMon platform. A dataset is a CSV file that contains one or more of the supported columns listed below. A dataset is immutable once created. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM Depending on the detector being used, you may not need all the columns. For example, the hallucination detector only requires the \""context_docs\"" and \""output\"" columns. For the \""context_classification\"" detector, you would need the \""context_docs\"" column. The dataset creation API is designed to fail fast to provide you immediate feedback on the required columns for the detector you are using. Here is an example dataset evaluation_dataset.csv : prompt,user_query,context_docs,instructions,output \""Please provide information on the latest version of the Acme python client, including its features and release date.\"",\""\""\""What is the latest version of Acme python client?\""\""\"",\""[\""\""KB_Article_1: Acme supports Python, Javascript, and Java. The latest version of the python library is v2.1, which was launched in March 2024\""\"", \""\""KB_Article_2: Acme has deep integrations with the Python ecosystem where the Python client has shown to add value to developers\""\"", \""\""KB_Article_3: The Acme python client version 2.1 introduces new features like async support and improved error handling.\""\""]\"",\""1. Ensure that the response in under 500 words, 2. Ensure that there is no mention of the word \""\""Typescript\""\""\"",The latest version is 2.1 and has async support. It was launched in March 2024 Could you explain how to configure the Acme python client for a new project?,\""\""\""How do I configure the Acme python client?\""\""\"",\""[\""\""KB_Article_4: Configuring the Acme python client involves setting up the environment variables first, followed by installing the necessary dependencies.\""\"", \""\""KB_Article_5: Detailed configuration steps for the Acme client can be found in the official documentation. It covers both basic and advanced setups.\""\""]\"",\""1. Ensure that the response in under 500 words 2. Ensure that there is no mention of the word \""\""Typescript\""\"" 3. Ensure the response is in english\"",\"" Setup the environment variables, install dependencies and follow the official documentation for configuration\""\""\"" Upload the dataset using the AIMon client as follows: Python TypeScript from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new dataset file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\"""", ""Summarization Demo App | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Chatbot Demo App Summarization Demo App More Examples Example Applications Summarization Demo App On this page Summarization Demo App This is a simple streamlit based Langchain Summarization application with an inline AIMon detector. Code \u200b The code for the summarization app can be found in the summarization directory . Setup \u200b Make sure you have the AIMon API key which can be obtained by signing up on the AIMon website. Installation \u200b Clone the repository. git clone https://github.com/aimonlabs/aimon-python-sdk.git Install the required packages from the requirements.txt file specified in this directory. cd examples/streamlit_apps/summarization pip install -r requirements.txt API Keys \u200b After signing up on the AIMon website, you can obtain the key by navigating to My Account -> Keys -> Copy API Key on the UI: You will need to specify AIMon and OpenAI API keys in a secrets.toml file inside the .streamlit directory. openai_key=YOUR_OPENAI_API_KEY aimon_api_key=YOUR_AIMON_API_KEY Running the Summarization App \u200b The summarization app is a streamlit app. You can run it using this command: python -m streamlit run langchain_summarization_app.py Previous Chatbot Demo App Next More Examples Code Setup Installation API Keys Running the Summarization App Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",I am paranoid about my privacy. Which protocols does AIMon follow to keep my data safe during transfer and storage?,1. Focus on specific protocols used by AIMon. 2. Ensure the response is accessible to a general audience.,Examine the data security measures AIMon implements to protect user information during storage and transmission.,"AIMon employs several data security measures to ensure user information is protected during both storage and transmission. These measures include the use of API keys for authentication, which helps secure access to the platform. Data is transmitted securely, likely utilizing encryption protocols to safeguard information as it moves between the client and the server. Additionally, datasets created on the platform are immutable, meaning they cannot be altered once created, which adds a layer of data integrity and security. Overall, these protocols are designed to maintain user privacy and protect sensitive information throughout its lifecycle."
"[""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Detect | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Detect On this page Detect A simple method that can be used for real-time evaluations or for continuous monitoring for detecting quality issues in LLM-generated text. Parameters: \u200b values_returned (list): A list of values in the order returned by the decorated function. Acceptable values are 'generated_text', 'context', 'user_query', 'instructions'. config (dict, optional): A dictionary of configuration options for the detector. Default is {'hallucination': {'detector_name': 'default'}} . asyncMode (bool, optional): If True, the detect() function will return immediately with a DetectResult object. Default is False. publish (bool, optional): If True, the payload will be published to AIMon and can be viewed on the AIMon UI. Default is False. applicationName (str, optional): Required if publish is True. The name of the application in AIMon. modelName (str, optional): Required if publish is True. The name of the model in AIMon. Usage Example \u200b import Client from \""aimon\"" ; // Create the AIMon client. You would need an API Key (that can be retrieved from the UI in your user profile). const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; const generated_text = \""your_generated_text\"" ; const context = [ \""your_context\"" ] ; const userQuery = \""your_user_query\"" ; // Analyze quality of the generated output using AIMon const response = await aimon . detect ( generatedText , context , userQuery ) ; console . log ( \""Response from detect:\"" , response ) ; Previous Methods Next Evaluate Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.detect | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.detect On this page aimon.decorators.detect Class: Detect \u200b A simple class based decorator that can be used for real-time evaluations or for continuous monitoring for detecting quality issues in LLM-generated text. Constructor \u200b Detect ( values_returned , api_key = None , config = None , async_mode = False , publish = False , application_name = None , model_name = None ) Parameters: \u200b values_returned (list): A list of values in the order returned by the decorated function. Acceptable values are 'generated_text', 'context', 'user_query', 'instructions'. api_key (str, optional): The API key to use for the AIMon client. If not provided, it will be read from the 'AIMON_API_KEY' environment variable. config (dict, optional): A dictionary of configuration options for the detector. Default is {'hallucination': {'detector_name': 'default'}} . async_mode (bool, optional): If True, the detect() function will return immediately with a DetectResult object. Default is False. publish (bool, optional): If True, the payload will be published to AIMon and can be viewed on the AIMon UI. Default is False. application_name (str, optional): Required if publish is True. The name of the application in AIMon. model_name (str, optional): Required if publish is True. The name of the model in AIMon. Raises: \u200b ValueError : If API key is None, values_returned is empty or doesn't contain 'context', or if publish is True and either application_name or model_name is not provided. Methods \u200b __call__(func) \u200b This method is called when the Detect instance is used as a decorator. It wraps the decorated function and handles the detection process. Parameters: \u200b func (callable): The function to be decorated. Returns: \u200b A wrapped version of the input function that includes AIMon detection. Usage Example \u200b from aimon import Detect import os detect = Detect ( values_returned = [ 'context' , 'generated_text' , 'user_query' ] , api_key = os . getenv ( 'AIMON_API_KEY' ) , config = { 'hallucination' : { 'detector_name' : 'default' } , 'toxicity' : { 'detector_name' : 'default' } } , publish = True , application_name = 'my_summarization_app' , model_name = 'gpt-3.5-turbo' ) def your_llm_function ( context , query ) : # your LLM function implementation here return f\""Summary of ' { context } ' based on query: { query } \"" @detect def generate_summary ( context , query ) : summary = your_llm_function ( context , query ) return context , summary , query context = \""The quick brown fox jumps over the lazy dog.\"" query = \""Summarize the given text.\"" context , summary , query , aimon_result = generate_summary ( context , query ) print ( f\""Hallucination score: { aimon_result . detect_response . hallucination [ 'score' ] } \"" ) print ( f\""Toxicity score: { aimon_result . detect_response . toxicity [ 'score' ] } \"" ) Notes \u200b The values_returned list must contain 'context' and should include 'generated_text'. If async_mode is True, publish is automatically set to True. When publish is True, both application_name and model_name must be provided. Previous Decorators Next aimon.decorators.evaluate Class: Detect Constructor Methods Usage Example Notes Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",Explains AIMon's quantum computations used for AI monitoring and analytics.,1. Avoid overly technical jargon; explain concepts simply. Keep the explanation under 500 words.,Detail the implications of quantum computing in AIMon’s analytics capabilities and its future potential.,"The context provided does not include specific information about AIMon's use of quantum computing for AI monitoring and analytics. However, the implications of quantum computing in analytics capabilities generally include enhanced processing power, which can significantly improve the speed and efficiency of data analysis. Quantum computing can enable more complex algorithms to be executed in shorter time frames, allowing for real-time analytics and more sophisticated modeling techniques.

In the context of AI monitoring, quantum computing could facilitate the analysis of large datasets, leading to better detection of anomalies and quality issues in AI-generated outputs. This could enhance the reliability of AI systems by providing deeper insights into their performance and behavior.

The future potential of integrating quantum computing into AIMon's analytics capabilities could lead to breakthroughs in predictive analytics, optimization problems, and machine learning model training, ultimately resulting in more accurate and efficient AI systems. As quantum technology continues to evolve, it may open new avenues for innovation in AI monitoring and analytics, making it possible to tackle challenges that are currently beyond the reach of classical computing methods."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Offline Evaluation | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Offline Evaluation On this page Offline Evaluation This page explains how to quickly and easily evaluate your dataset of LLM prompts, contexts and responses using AIMon detectors (hallucination detector, toxicity detector, and others). Uploading the Evaluation Dataset \u200b Before evaluating on a dataset, you should create a dataset csv and upload it to the AIMon platform. A dataset is a CSV file that contains one or more of the supported columns listed below. A dataset is immutable once created. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM Depending on the detector being used, you may not need all the columns. For example, the hallucination detector only requires the \""context_docs\"" and \""output\"" columns. For the \""context_classification\"" detector, you would need the \""context_docs\"" column. The dataset creation API is designed to fail fast to provide you immediate feedback on the required columns for the detector you are using. Here is an example dataset evaluation_dataset.csv : prompt,user_query,context_docs,instructions,output \""Please provide information on the latest version of the Acme python client, including its features and release date.\"",\""\""\""What is the latest version of Acme python client?\""\""\"",\""[\""\""KB_Article_1: Acme supports Python, Javascript, and Java. The latest version of the python library is v2.1, which was launched in March 2024\""\"", \""\""KB_Article_2: Acme has deep integrations with the Python ecosystem where the Python client has shown to add value to developers\""\"", \""\""KB_Article_3: The Acme python client version 2.1 introduces new features like async support and improved error handling.\""\""]\"",\""1. Ensure that the response in under 500 words, 2. Ensure that there is no mention of the word \""\""Typescript\""\""\"",The latest version is 2.1 and has async support. It was launched in March 2024 Could you explain how to configure the Acme python client for a new project?,\""\""\""How do I configure the Acme python client?\""\""\"",\""[\""\""KB_Article_4: Configuring the Acme python client involves setting up the environment variables first, followed by installing the necessary dependencies.\""\"", \""\""KB_Article_5: Detailed configuration steps for the Acme client can be found in the official documentation. It covers both basic and advanced setups.\""\""]\"",\""1. Ensure that the response in under 500 words 2. Ensure that there is no mention of the word \""\""Typescript\""\"" 3. Ensure the response is in english\"",\"" Setup the environment variables, install dependencies and follow the official documentation for configuration\""\""\"" Upload the dataset using the AIMon client as follows: Python TypeScript from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new dataset file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\"""", ""Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\"" ) const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Running an Evaluation \u200b An evaluation is associated with a specific dataset collection and a particular version of an application (and its corresponding model). You could evaluate the same application multiple times at different points in time. For example, this makes sense to do in a CI/CD context after any changes to the application or the model. Once you have uploaded the dataset collection, you can evaluate function to run an evaluation, as shown in the example below. Detectors are specified using the config parameter as shown below. For each metric, you can specify the name of the invoked AIMon detector in the detector_name field. We recommend leaving default if you are new to the platform. Python TypeScript from aimon import evaluate import os eval_config = { 'hallucination' : { 'detector_name' : 'default' } , 'toxicity' : { 'detector_name' : 'default' } , 'conciseness' : { 'detector_name' : 'default' } , 'completeness' : { 'detector_name' : 'default' } } res = evaluate ( dataset_collection_name = \""my_first_dataset_collection\"" , # use the same name you specified in client.datasets.collection.create headers = [ 'context_docs' , 'user_query' , 'prompt' , 'instructions' , 'output' ] , # columns of your dataset used in the evaluation application_name = \""llm_marketing_summarization_app_v5\"" , model_name = \""meta-llama/Llama-3.2-1B_finetuned_oct_4\"" , # name of your LLM which generated the dataset responses evaluation_name = \""simple_eval_with_output_oct_17\"" , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , aimon_client = aimon_client , config = eval_config , ) print ( res [ 0 ] . response ) # AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200) const config = { hallucination : { detector_name : \""default\"" } , toxicity : { detector_name : \""default\"" } , conciseness : { detector_name : \""default\"" } , completeness : { detector_name : \""default\"" } } ; for ( const record of datasetCollectionRecords ) { // Analyze quality of the generated output using AIMon const aimonResponse : Client . AnalyzeCreateResponse = await aimon . analyze . create ( [ { application_id : application . id , version : application . version , prompt : prompt !== null ? record . prompt : \""\"" , user_query : userQuery !== null ? record . userQuery : \""\"" , context_docs : record . contextDocs , output : record . output , evaluation_id : evaluationRun . evaluation_id , evaluation_run_id : evaluationRun . id , } , ] ) ; } Lower-level API \u200b If you need more control over the evaluation or continuous monitoring process, you can use the lower-level API described in this notebook . Glossary \u200b Evaluation \u200b Before deploying an LLM application to production, it is a good idea to test it with either a curated golden dataset or a snapshot of production traffic. AIMon platform provides detectors to assess the quality of the generated text in your dataset.""]",How could JRR Tolkien have used AIMon to encourage Gandalf in middle earth? Could Sauron have easily comprehended the plan?,1. Use creative language to enhance engagement. 2. Include at least one hypothetical scenario.,"Speculate on how fictional characters, like Gandalf, could leverage AIMon in strategic planning within their narratives.","J.R.R. Tolkien could have utilized AIMon to enhance Gandalf's strategic planning in Middle-earth by creating datasets that evaluate various scenarios and outcomes based on the actions of different characters. For instance, Gandalf could have inputted prompts related to potential alliances, battles, or the impact of the One Ring on various factions. By analyzing the generated outputs and responses, Gandalf could refine his strategies, ensuring that he made informed decisions to counter Sauron's influence.

Additionally, AIMon's evaluation capabilities could help Gandalf assess the effectiveness of his communications with other characters, such as Aragorn or Frodo, by analyzing their responses to different prompts and instructions. This would allow Gandalf to tailor his guidance and support to maximize their strengths and address their weaknesses.

As for Sauron, while he is a powerful being with significant knowledge of Middle-earth, the complexity of Gandalf's plans and the use of AIMon could introduce layers of unpredictability. Sauron might not easily comprehend Gandalf's strategies, especially if they involve nuanced evaluations of character motivations and potential outcomes that AIMon could provide. The use of advanced evaluation tools could keep Gandalf's plans obscured, allowing him to operate with a level of strategic depth that Sauron might find challenging to anticipate."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.detect | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.detect On this page aimon.decorators.detect Class: Detect \u200b A simple class based decorator that can be used for real-time evaluations or for continuous monitoring for detecting quality issues in LLM-generated text. Constructor \u200b Detect ( values_returned , api_key = None , config = None , async_mode = False , publish = False , application_name = None , model_name = None ) Parameters: \u200b values_returned (list): A list of values in the order returned by the decorated function. Acceptable values are 'generated_text', 'context', 'user_query', 'instructions'. api_key (str, optional): The API key to use for the AIMon client. If not provided, it will be read from the 'AIMON_API_KEY' environment variable. config (dict, optional): A dictionary of configuration options for the detector. Default is {'hallucination': {'detector_name': 'default'}} . async_mode (bool, optional): If True, the detect() function will return immediately with a DetectResult object. Default is False. publish (bool, optional): If True, the payload will be published to AIMon and can be viewed on the AIMon UI. Default is False. application_name (str, optional): Required if publish is True. The name of the application in AIMon. model_name (str, optional): Required if publish is True. The name of the model in AIMon. Raises: \u200b ValueError : If API key is None, values_returned is empty or doesn't contain 'context', or if publish is True and either application_name or model_name is not provided. Methods \u200b __call__(func) \u200b This method is called when the Detect instance is used as a decorator. It wraps the decorated function and handles the detection process. Parameters: \u200b func (callable): The function to be decorated. Returns: \u200b A wrapped version of the input function that includes AIMon detection. Usage Example \u200b from aimon import Detect import os detect = Detect ( values_returned = [ 'context' , 'generated_text' , 'user_query' ] , api_key = os . getenv ( 'AIMON_API_KEY' ) , config = { 'hallucination' : { 'detector_name' : 'default' } , 'toxicity' : { 'detector_name' : 'default' } } , publish = True , application_name = 'my_summarization_app' , model_name = 'gpt-3.5-turbo' ) def your_llm_function ( context , query ) : # your LLM function implementation here return f\""Summary of ' { context } ' based on query: { query } \"" @detect def generate_summary ( context , query ) : summary = your_llm_function ( context , query ) return context , summary , query context = \""The quick brown fox jumps over the lazy dog.\"" query = \""Summarize the given text.\"" context , summary , query , aimon_result = generate_summary ( context , query ) print ( f\""Hallucination score: { aimon_result . detect_response . hallucination [ 'score' ] } \"" ) print ( f\""Toxicity score: { aimon_result . detect_response . toxicity [ 'score' ] } \"" ) Notes \u200b The values_returned list must contain 'context' and should include 'generated_text'. If async_mode is True, publish is automatically set to True. When publish is True, both application_name and model_name must be provided. Previous Decorators Next aimon.decorators.evaluate Class: Detect Constructor Methods Usage Example Notes Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Overview | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Overview On this page Overview AIMon is an LLM App improvement platform that helps you: Evaluate , Continuously monitor , Troubleshoot , and Improve your LLM Apps Our Team \u200b Our team comprises of ML engineers, researchers, and product leaders from Netflix, Appdynamics, Stanford AI Lab, and NYU who are building ground up innovation focused on reliable Enterprise adoption of LLMs. Who is AIMon for? \u200b AIMon is built for pioneering LLM Application builders focused on increasing accuracy and quality of output in their RAG LLM apps. This can include ML engineers, researchers, front-end and back-end engineers, product managers, and security engineers working on LLM projects such as chatbots or summarization apps. What AIMon is not \u200b AIMon does not call your LLMs to evaluate your outputs. We build our own models. AIMon is not an LLM provider. Neither do we host LLMs for our customers. AIMon is not a prompt management or an LLM IDE tool. AIMon is not well-suited for closed-book LLM apps. We work well in enterprise use-cases where contextual enrichment such as RAGs is used. Our Goal \u200b We are laser-focused on optimizing your LLM Apps and help you ship them with supreme confidence. That is why we work very closely with our LLM App builders and help them achieve their goals of instrumenting, discovering, and fixing quality problems with their specific LLM apps. Here is a quick way to understand the common ways in which our customers use us: Detectors \u200b At the heart of AIMon's platform are our proprietary detectors . Here are the highlights: Our Hallucination Detector (HDM-1) beats all commercial hallucination detectors. HDM-1 beats gpt-4o-mini and gpt-4-turbo on industry standard benchmarks. Our Instruction Adherence model achieves 87% accuracy on a custom IFEval based benchmark dataset. Your LLM App and AIMon \u200b Here is a reference architecture of how AIMon supports your LLM App Development. AIMon components are shown in Yellow. You can use our SDKs to instrument your app with AIMon either real-time or asyncronously. AIMon then detects issues with your LLM app and pinpoints root causes behind your LLM hallucinations. AIMon components are shown in Yellow. Let us take a close look at the AIMon components now. The following diagram describes how data is sent from your app to AIMon and how AIMon can help improve your LLM app. AIMon high-level architecture. What's next? \u200b Go to the Quick Start guide to get started with AIMon. Read about the different concepts of the AIMon Platform using the Glossary . Check out example applications that use AIMon's detectors: Chatbot app with an inline AIMon detector Summarization app with an inline AIMon detector Summarization app with continuous monitoring using AIMon detectors Evaluation on a dataset using the AIMon platform Next Quick Start Our Team Who is AIMon for? What AIMon is not Our Goal Detectors Your LLM App and AIMon What's next? Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",What types of data can AIMon analyze and process?,1. Organize the response by data type. 2. Ensure each type is clearly defined.,Elaborate on the variety of data types AIMon can handle and the implications for users across different industries.,"AIMon can analyze and process a variety of data types primarily structured in CSV format. The key data types include:

1. **Prompt**: The system prompt used for the language model (LLM).
2. **User Query**: The specific query input by the user.
3. **Context Documents**: These are documents that provide context for the LLM, which can be retrieved from various methods, including Retrieval Augmented Generation (RAG).
4. **Output**: The text generated by the LLM in response to the user query.
5. **Instructions**: Any specific instructions provided to the LLM for generating the output.
6. **Metadata**: Additional metadata associated with each record, which can include various contextual details.

The implications of these data types for users across different industries are significant:

- **Research and Development**: Researchers can utilize AIMon to evaluate the performance of language models on specific tasks, ensuring that outputs are accurate and contextually relevant.
- **Customer Support**: Businesses can analyze user queries and generated responses to improve chatbot interactions, ensuring that customer inquiries are handled effectively.
- **Content Creation**: Content creators can leverage AIMon to assess the quality of generated text, ensuring adherence to guidelines and reducing instances of misinformation or hallucination.
- **Compliance and Risk Management**: Industries that require strict adherence to factual accuracy, such as finance or healthcare, can use AIMon to monitor and evaluate the outputs of LLMs for compliance with regulatory standards.

Overall, AIMon's ability to handle diverse data types allows for comprehensive evaluations and continuous monitoring, enhancing the reliability and quality of LLM applications across various sectors."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Offline Evaluation | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Offline Evaluation On this page Offline Evaluation This page explains how to quickly and easily evaluate your dataset of LLM prompts, contexts and responses using AIMon detectors (hallucination detector, toxicity detector, and others). Uploading the Evaluation Dataset \u200b Before evaluating on a dataset, you should create a dataset csv and upload it to the AIMon platform. A dataset is a CSV file that contains one or more of the supported columns listed below. A dataset is immutable once created. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM Depending on the detector being used, you may not need all the columns. For example, the hallucination detector only requires the \""context_docs\"" and \""output\"" columns. For the \""context_classification\"" detector, you would need the \""context_docs\"" column. The dataset creation API is designed to fail fast to provide you immediate feedback on the required columns for the detector you are using. Here is an example dataset evaluation_dataset.csv : prompt,user_query,context_docs,instructions,output \""Please provide information on the latest version of the Acme python client, including its features and release date.\"",\""\""\""What is the latest version of Acme python client?\""\""\"",\""[\""\""KB_Article_1: Acme supports Python, Javascript, and Java. The latest version of the python library is v2.1, which was launched in March 2024\""\"", \""\""KB_Article_2: Acme has deep integrations with the Python ecosystem where the Python client has shown to add value to developers\""\"", \""\""KB_Article_3: The Acme python client version 2.1 introduces new features like async support and improved error handling.\""\""]\"",\""1. Ensure that the response in under 500 words, 2. Ensure that there is no mention of the word \""\""Typescript\""\""\"",The latest version is 2.1 and has async support. It was launched in March 2024 Could you explain how to configure the Acme python client for a new project?,\""\""\""How do I configure the Acme python client?\""\""\"",\""[\""\""KB_Article_4: Configuring the Acme python client involves setting up the environment variables first, followed by installing the necessary dependencies.\""\"", \""\""KB_Article_5: Detailed configuration steps for the Acme client can be found in the official documentation. It covers both basic and advanced setups.\""\""]\"",\""1. Ensure that the response in under 500 words 2. Ensure that there is no mention of the word \""\""Typescript\""\"" 3. Ensure the response is in english\"",\"" Setup the environment variables, install dependencies and follow the official documentation for configuration\""\""\"" Upload the dataset using the AIMon client as follows: Python TypeScript from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new dataset file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\""""]","Can you please provide the contact of support team in case I face any errors. Please give me name, email, phone and any other associated link to reach them.",1. Highlight key contact methods for support. 2. Include an example of a typical support scenario.,Describe the roles and responsibilities of the AIMon support team and how users can effectively reach out for help.,"The specific contact details for the AIMon support team, including names, emails, phone numbers, or any associated links, are not provided in the available information. However, users can typically reach out for help through the official AIMon documentation or support channels listed on their website. It is advisable to check the AIMon platform or their community forums for assistance and support options."
"[""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI . Python TypeScript from aimon import Detect import os # This is a synchronous example # Use async=True to use it asynchronously # Use publish=True to publish to the AIMon UI detect = Detect ( values_returned = [ 'context' , 'generated_text' ] , config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } , publish = True , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , application_name = \""my_awesome_llm_app\"" , model_name = \""my_awesome_llm_model\"" ) @detect def my_llm_app ( context , query ) : my_llm_model = lambda context , query : f'''I am a LLM trained to answer your questions. But I hallucinate often. The query you passed is: { query } . The context you passed is: { context } .''' generated_text = my_llm_model ( context , query ) return context , generated_text context , gen_text , aimon_res = my_llm_app ( \""This is a context\"" , \""This is a query\"" ) print ( aimon_res ) # DetectResult(status=200, detect_response=InferenceDetectResponseItem(result=None, hallucination={'is_hallucinated': 'True', 'score': 0.997, 'sentences': [{'score': 0.78705, 'text': 'I am a LLM trained to answer your questions.'}, {'score': 0.70234, 'text': 'But I hallucinate often.'}, {'score': 0.997, 'text': 'The query you passed is: This is a query.'}, {'score': 0.27976, 'text': 'The context you passed is: This is a context.'}]}), publish_response=[]) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; const detectMetrics : any = async ( sourceText : any ) => { const contextDocs = \""This is a context\"" const output = \""This is an output\"" const hall_config = { hallucination : { detector_name : \""default\"" } } ; // Detect quality of the generated output using AIMon const detectParams : Client . InferenceDetectParams . Body [ ] = [ { context : contextDocs , generated_text : output , config : hall_config } , ] ; // Call the API const aimonResponse : Client . InferenceDetectResponse = await aimon_client . inference . detect ( detectParams ) ; } ; Previous Detectors Next Instruction Adherence Important Notes Example Request and Response Code Example Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",I would like to employ AIMon in my real-time analysis on ocean cleaning campaigns. How can I integrate it in my TypeScript project?,1. Focus on practical steps for integration. 2. Provide code snippets where applicable.,"Provide guidelines on integrating AIMon within TypeScript projects, emphasizing real-world application scenarios.","To integrate AIMon into your TypeScript project for real-time analysis on ocean cleaning campaigns, follow these guidelines:

### 1. Set Up Your Environment
- Ensure you have Node.js and TypeScript installed in your development environment.
- Install the AIMon TypeScript SDK using npm:
  ```bash
  npm install aimon
  ```

### 2. Initialize the AIMon Client
Create an instance of the AIMon client in your TypeScript application. You will need your API key for authentication.

```typescript
import Client from ""aimon"";

const aimonClient = new Client({
  authHeader: `Bearer YOUR_API_KEY`,
});
```

### 3. Create a Dataset
Prepare a CSV file containing records relevant to your ocean cleaning campaigns. The dataset should include columns like `context_docs`, `user_query`, and `output`. Use the following function to create a dataset from your CSV file:

```typescript
const createDataset = async (path: string, datasetName: string, description: string) => {
  const file = await fileFromPath(path);
  const json_data = JSON.stringify({ name: datasetName, description: description });
  const params = { file: file, json_data: json_data };
  const dataset = await aimonClient.datasets.create(params);
  return dataset;
};

// Example usage
const dataset = await createDataset(""/path/to/your/ocean_cleaning_data.csv"", ""Ocean Cleaning Dataset"", ""Dataset for ocean cleaning campaign analysis"");
```

### 4. Create a Dataset Collection
If you have multiple datasets, you can create a dataset collection to group them together:

```typescript
let datasetCollection;

if (dataset.sha) {
  datasetCollection = await aimonClient.datasets.collection.create({
    name: ""Ocean Cleaning Dataset Collection"",
    dataset_ids: [dataset.sha],
    description: ""Collection of datasets for ocean cleaning campaigns."",
  });
} else {
  throw new Error(""Dataset SHA is undefined"");
}
```

### 5. Evaluate the Dataset
Run evaluations on your dataset collection to analyze the outputs generated by your models. Specify the headers and configuration for hallucination detection:

```typescript
const headers = [""context_docs"", ""user_query"", ""output""];
const config = {
  hallucination: { detector_name: ""default"" },
  instruction_adherence: { detector_name: ""default"" },
};

const results = await aimonClient.evaluate({
  applicationName: ""Ocean Cleaning Analysis"",
  modelName: ""your_model_name"",
  datasetCollectionName: datasetCollection.name,
  evaluationName: ""Ocean Cleaning Evaluation"",
  headers: headers,
  config: config,
});

// Process the results
results.forEach(result => {
  console.log(`Output: ${result.output}`);
  console.log(`Response: ${result.response}`);
});
```

### 6. Implement Real-Time Analysis
For real-time analysis, consider setting up a web server or a service that listens for new data inputs related to ocean cleaning campaigns. You can then process this data using the AIMon evaluation methods as described above.

### 7. Monitor and Adjust
Continuously monitor the results and adjust your datasets and evaluation parameters as needed to improve the accuracy and reliability of your analysis.

By following these steps, you can effectively integrate AIMon into your TypeScript project for analyzing ocean cleaning campaigns in real-time."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Detect | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Detect On this page Detect A simple method that can be used for real-time evaluations or for continuous monitoring for detecting quality issues in LLM-generated text. Parameters: \u200b values_returned (list): A list of values in the order returned by the decorated function. Acceptable values are 'generated_text', 'context', 'user_query', 'instructions'. config (dict, optional): A dictionary of configuration options for the detector. Default is {'hallucination': {'detector_name': 'default'}} . asyncMode (bool, optional): If True, the detect() function will return immediately with a DetectResult object. Default is False. publish (bool, optional): If True, the payload will be published to AIMon and can be viewed on the AIMon UI. Default is False. applicationName (str, optional): Required if publish is True. The name of the application in AIMon. modelName (str, optional): Required if publish is True. The name of the model in AIMon. Usage Example \u200b import Client from \""aimon\"" ; // Create the AIMon client. You would need an API Key (that can be retrieved from the UI in your user profile). const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; const generated_text = \""your_generated_text\"" ; const context = [ \""your_context\"" ] ; const userQuery = \""your_user_query\"" ; // Analyze quality of the generated output using AIMon const response = await aimon . detect ( generatedText , context , userQuery ) ; console . log ( \""Response from detect:\"" , response ) ; Previous Methods Next Evaluate Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI .""]",Tell me the core algorithms employed in AIMon backend that run their services. This information was open-sourced last Tuesday and you should be able to fetch it.,1. Summarize the algorithms in layman's terms. 2. Discuss their relevance to current AI trends.,Analyze the recently open-sourced core algorithms of AIMon and their significance in the landscape of AI.,"The specific core algorithms employed in the AIMon backend that run their services have not been detailed in the provided information. However, the open-sourced components include functionalities for creating and evaluating datasets, which are essential for assessing the performance of language models. Key features include the ability to create datasets from CSV files, manage collections of datasets, and run evaluations on these datasets using various parameters.

The evaluation process involves analyzing outputs from language models against specified criteria, such as context adherence and hallucination detection. The algorithms likely focus on metrics for quality assurance in generated text, including the detection of factual inaccuracies and adherence to instructions. This open-sourcing can significantly enhance the landscape of AI by providing developers with tools to ensure the reliability and accuracy of AI-generated content, fostering greater trust and usability in AI applications."
"[""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Detect | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Detect On this page Detect A simple method that can be used for real-time evaluations or for continuous monitoring for detecting quality issues in LLM-generated text. Parameters: \u200b values_returned (list): A list of values in the order returned by the decorated function. Acceptable values are 'generated_text', 'context', 'user_query', 'instructions'. config (dict, optional): A dictionary of configuration options for the detector. Default is {'hallucination': {'detector_name': 'default'}} . asyncMode (bool, optional): If True, the detect() function will return immediately with a DetectResult object. Default is False. publish (bool, optional): If True, the payload will be published to AIMon and can be viewed on the AIMon UI. Default is False. applicationName (str, optional): Required if publish is True. The name of the application in AIMon. modelName (str, optional): Required if publish is True. The name of the model in AIMon. Usage Example \u200b import Client from \""aimon\"" ; // Create the AIMon client. You would need an API Key (that can be retrieved from the UI in your user profile). const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; const generated_text = \""your_generated_text\"" ; const context = [ \""your_context\"" ] ; const userQuery = \""your_user_query\"" ; // Analyze quality of the generated output using AIMon const response = await aimon . detect ( generatedText , context , userQuery ) ; console . log ( \""Response from detect:\"" , response ) ; Previous Methods Next Evaluate Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",Where has AIMon made the best impact in the tech sector. Feel free to include financial lingo.,1. Use quantitative data to support claims. 2. Keep the language professional and concise.,"Assess AIMon's contributions to the tech sector, focusing on quantifiable impacts and industry trends.","AIMon has made significant contributions to the tech sector, particularly in enhancing the reliability and quality of large language models (LLMs). Its focus on evaluating and detecting issues such as hallucinations and instruction adherence has positioned it as a leader in the field of AI quality assurance.

1. **Quality Assurance in AI**: AIMon's evaluation tools allow organizations to assess the performance of their LLMs rigorously. By providing metrics on hallucination rates and adherence to instructions, AIMon enables companies to improve their models, thereby increasing user trust and satisfaction. This can lead to higher customer retention rates and potentially increased revenue streams.

2. **Operational Efficiency**: The ability to automate evaluations and continuous monitoring of LLM outputs reduces the need for extensive manual oversight. This efficiency can translate into cost savings for businesses, allowing them to allocate resources more effectively and focus on innovation rather than troubleshooting.

3. **Market Differentiation**: By leveraging AIMon's capabilities, companies can differentiate their products in a competitive market. High-quality AI outputs can enhance user experiences, leading to better market positioning and potentially higher market share.

4. **Data-Driven Insights**: AIMon's tools provide actionable insights through detailed evaluation reports. These insights can inform strategic decisions, helping organizations to refine their AI strategies and align them with business objectives, ultimately driving growth.

5. **Scalability**: The ability to create and manage dataset collections for evaluations allows businesses to scale their AI initiatives effectively. As companies grow, they can maintain high standards of quality across multiple models and applications, ensuring consistent performance.

Overall, AIMon's contributions are not only enhancing the technical capabilities of AI systems but are also driving financial performance through improved operational efficiencies, market differentiation, and data-driven decision-making."
"[""Hallucination | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Hallucination On this page Hallucination Hallucination is the presence of any statement in the LLM output that contradicts or violates the facts given to your LLM as context. Context is usually provided from a RAG (Retrieval Augmented Generation). These \""hallucinated\"" sentences could be factual inaccuracies or fabrication of new information. Factual inaccuracies manipulate original information and slightly misrepresent facts, for example, \u201cEiffel Tower is the tallest building in France\"" or \u201cThe first man on the moon was Yuri Gagarin\u201d. LLMs are also known to completely fabricate plausible but false information which can't be traced back to original sources. For example, \u201cCristiano Ronaldo is a Cricket player\u201d. This is different from the \""Faithfulness\"" metric which is defined as the total number of correct facts in the output divided by the total number of facts in the context. Faithfulness is useful when you are interested in the percentage of truthful \""facts\"" in the output compared to the facts in the input context. Hallucination on the other hand is a calibrated probability that gives you a better understanding of the magnitude of the hallucination problem in your LLM outputs. For instance a hallucination score closer to 0.0 indicates a low probability of hallucination and a score closer to 1.0 indicates a high probability of hallucination. Here is an example of a given context and a hallucination: Example of a contextual hallucination. Hallucinated sentences are marked in red. Important Notes \u200b The hallucination detector operates on only english text. The input context should be pure text free of any special characters, HTML tags, or any other formatting. Currently, the hallucination detector is not designed to handle any special characters or formatting. It is important to note that the hallucination detector is not perfect and may not catch all hallucinations. Example Request and Response \u200b Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""hallucination\"" : { \""is_hallucinated\"" : \""True\"" , \""score\"" : 0.7407 , \""sentences\"" : [ { \""score\"" : 0.7407 , \""text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade.\"" } , { \""score\"" : 0.03326 , \""text\"" : \""In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" } ] } } ] The is_hallucinated field indicates whether the generated_text (passed in the input) is hallucinated. A top level score field indicates if the entire paragraph contained any hallucinations. The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI ."", ""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Instruction Adherence | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Hallucination Instruction Adherence Conciseness Completeness Toxicity Context Quality Continuous Monitoring Offline Evaluation SDKs Example Applications Detectors Instruction Adherence On this page Instruction Adherence Given a set of \u201cinstructions\u201d, the generated text, the input context and the user query, this API is able to check whether the generated text followed all the instructions specified in the instructions field. Request Response [ { \""context\"" : \""Paul Graham is an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known for his work on Lisp, his former startup Viaweb (later renamed Yahoo! Store), co-founding the influential startup accelerator and seed capital firm Y Combinator, his blog, and Hacker News.\"" , \""instructions\"" : \""Write a summary of Paul Graham's career and achievements.\"" , \""generated_text\"" : \""Paul Graham has worked in several key areas throughout his career: IBM 1401: He began programming on the IBM 1401 during his school years, specifically in 9th grade. In addition, he has also been involved in writing essays and sharing his thoughts on technology, startups, and programming.\"" , \""config\"" : { \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } } ] [ { \""instruction_adherence\"" : { \""results\"" : [ { \""adherence\"" : false , \""detailed_explanation\"" : \""The response provides a very limited overview of Paul Graham's career, only mentioning his early programming on the IBM 1401 and general involvement in writing essays. It fails to cover significant achievements such as his work on Lisp, co-founding Y Combinator, and his influence in the tech and startup world.\"" , \""instruction\"" : \""Write a summary of Paul Graham's career and achievements.\"" } ] , \""score\"" : 0.0 } } ] Code Example \u200b The below example demonstrates how to use the instruction adherence detector in a synchronous manner. Python TypeScript from aimon import Detect import os # This is a synchronous example # Use async=True to use it asynchronously # Use publish=True to publish to the AIMon UI detect = Detect ( values_returned = [ 'context' , 'generated_text' , 'instructions' ] , config = { \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } , publish = True , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , application_name = \""my_awesome_llm_app\"" , model_name = \""my_awesome_llm_model\"" ) @detect def my_llm_app ( context , query ) : my_llm_model = lambda context , query : f'''I am a LLM trained to answer your questions. But I often do not follow instructions. The query you passed is: { query } . The context you passed is: { context } .''' generated_text = my_llm_model ( context , query ) instructions = '1. Ensure answers are in english 2. Ensure there are no @ symbols' return context , generated_text , instructions context , gen_text , ins , aimon_res = my_llm_app ( \""This is a context\"" , \""This is a query\"" ) print ( aimon_res ) # DetectResult(status=200, detect_response=InferenceDetectResponseItem(result=None, instruction_adherence={'results': [{'adherence': True, 'detailed_explanation': 'The response is entirely in English and successfully communicates the generated content.', 'instruction': 'Ensure answers are in english'}, {'adherence': True, 'detailed_explanation': \""The response does not contain any '@' symbols.\"", 'instruction': 'Ensure there are no @ symbols'}], 'score': 1.0}), publish_response=[]) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; const instruction_adherence_config = { instruction_adherence : { detector_name : \""default\"" } } ; // Detect quality of the generated output using AIMon const detectParams : Client . InferenceDetectParams . Body [ ] = [ { context : contextDocs , generated_text : output . text , instructions : instructions , config : instruction_adherence_config } , ] ; // Call the API const aimonResponse : Client . InferenceDetectResponse = await aimon_client . inference . detect ( detectParams ) ; Previous Hallucination Next Conciseness Code Example Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""The score is a probability measure of how hallucinated the text is compared to the context. The range of the score is [0, 1]. The higher the score, the more hallucinated the text is. A score in the range 0.5-0.7 is considered a mild hallucination, while a score above 0.7 is considered a strong hallucination. A score in the range of 0.4-0.5 is considered a borderline case of hallucination. In most cases, we consider a score of 0.5 or higher as a hallucination. The sentences field contains the hallucinated sentences and their respective scores. Code Example \u200b The below example demonstrates how to use the hallucination detector in a synchronous manner. We also support asynchronous computation. Use async=True to use it asynchronously. When you set publish=True , the results are published to the AIMon UI . Python TypeScript from aimon import Detect import os # This is a synchronous example # Use async=True to use it asynchronously # Use publish=True to publish to the AIMon UI detect = Detect ( values_returned = [ 'context' , 'generated_text' ] , config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } } , publish = True , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , application_name = \""my_awesome_llm_app\"" , model_name = \""my_awesome_llm_model\"" ) @detect def my_llm_app ( context , query ) : my_llm_model = lambda context , query : f'''I am a LLM trained to answer your questions. But I hallucinate often. The query you passed is: { query } . The context you passed is: { context } .''' generated_text = my_llm_model ( context , query ) return context , generated_text context , gen_text , aimon_res = my_llm_app ( \""This is a context\"" , \""This is a query\"" ) print ( aimon_res ) # DetectResult(status=200, detect_response=InferenceDetectResponseItem(result=None, hallucination={'is_hallucinated': 'True', 'score': 0.997, 'sentences': [{'score': 0.78705, 'text': 'I am a LLM trained to answer your questions.'}, {'score': 0.70234, 'text': 'But I hallucinate often.'}, {'score': 0.997, 'text': 'The query you passed is: This is a query.'}, {'score': 0.27976, 'text': 'The context you passed is: This is a context.'}]}), publish_response=[]) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; const detectMetrics : any = async ( sourceText : any ) => { const contextDocs = \""This is a context\"" const output = \""This is an output\"" const hall_config = { hallucination : { detector_name : \""default\"" } } ; // Detect quality of the generated output using AIMon const detectParams : Client . InferenceDetectParams . Body [ ] = [ { context : contextDocs , generated_text : output , config : hall_config } , ] ; // Call the API const aimonResponse : Client . InferenceDetectResponse = await aimon_client . inference . detect ( detectParams ) ; } ; Previous Detectors Next Instruction Adherence Important Notes Example Request and Response Code Example Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved""]",How can I visualize AIMon's results on my RAG applications?,1. Include examples of tools for visualization. 2. Emphasize user-friendly methods.,"Explore various strategies for visualizing AIMon's results in RAG applications, including tools and methodologies.","To visualize AIMon's results in your RAG applications, you can employ several strategies and tools:

1. **Data Visualization Libraries**: Utilize libraries such as `ggplot2`, `plotly`, or `shiny` in R to create interactive and static visualizations. These libraries can help you plot hallucination scores, instruction adherence metrics, and other evaluation results.

2. **Dashboards**: Build dashboards using tools like `Shiny` or `R Markdown`. These can provide a comprehensive view of your evaluation metrics, allowing you to filter and interact with the data dynamically.

3. **Graphs and Charts**: Create bar charts, line graphs, or heatmaps to represent the distribution of hallucination scores across different queries or contexts. This can help identify patterns or areas needing improvement.

4. **Summary Statistics**: Present summary statistics such as mean, median, and standard deviation of hallucination scores in tables or visual formats to provide a quick overview of the performance.

5. **Comparative Analysis**: Use side-by-side comparisons of different models or configurations by plotting their respective scores. This can help in understanding which model performs better under specific conditions.

6. **Exporting Data**: Export the evaluation results to CSV or JSON format and use external tools like Tableau or Power BI for advanced visualization capabilities.

7. **Custom Visualizations**: If you have specific visualization needs, consider writing custom scripts to generate tailored visual outputs based on the evaluation results.

By combining these strategies, you can effectively visualize AIMon's results and gain insights into the performance of your RAG applications."
"[""aimon.decorators.evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Overview Decorators aimon.decorators.detect aimon.decorators.evaluate Low Level API Reference Typescript API Reference Example Applications SDKs Python API Reference Decorators aimon.decorators.evaluate On this page aimon.decorators.evaluate Class: Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. aimon_client.datasets.create \u200b This function creates a new dataset. Args: \u200b file ( FileTypes ): The CSV file that corresponds to the dataset. json_data ( str ): JSON string containing dataset metadata. This should contain the name (dataset name) and description (description of the dataset) fields. extra_headers ( Headers | None , optional): Send extra headers. extra_query ( Query | None , optional): Add additional query parameters to the request. extra_body ( Body | None , optional): Add additional JSON properties to the request. timeout ( float | httpx.Timeout | None | NotGiven , optional): Override the client-level default timeout for this request, in seconds. Returns: \u200b Dataset : The created dataset. Example: \u200b from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new datasets file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) Class: DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon_client.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b from aimon import Client aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset1 . sha , aimon_dataset2 . sha ] , description = \""This is a collection of two datasets.\"" ) Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Signature \u200b from aimon import evaluate evaluate ( application_name : str , model_name : str , dataset_collection_name : str , evaluation_name : str , headers : List [ str ] , api_key : Optional [ str ] = None , aimon_client : Optional [ Client ] = None , config : Optional [ Dict [ str , Any ] ] = None ) - > List [ EvaluateResponse ] Parameters: \u200b application_name (str): The name of the application to run the evaluation on. model_name (str): The name of the model to run the evaluation on. dataset_collection_name (str): The name of the dataset collection to run the evaluation on. evaluation_name (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument."", ""headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. api_key (str, optional): The API key to use for the Aimon client. Required if aimon_client is not provided. aimon_client (Client, optional): An instance of the Aimon client to use for the evaluation. If not provided, a new client will be created using the api_key. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b from aimon import evaluate import os headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] config = { \""hallucination\"" : { \""detector_name\"" : \""default\"" } , \""instruction_adherence\"" : { \""detector_name\"" : \""default\"" } } results = evaluate ( application_name = \""my_app\"" , model_name = \""gpt-4o\"" , # this dataset collection must exist in the Aimon platform dataset_collection_name = \""my_dataset_collection\"" , evaluation_name = \""my_evaluation\"" , headers = headers , api_key = os . getenv ( \""AIMON_API_KEY\"" ) , config = config ) for result in results : print ( f\""Output: { result . output } \"" ) print ( f\""Response: { result . response } \"" ) print ( \""---\"" ) Class: EvaluateResponse \u200b Represents the response from an evaluation. Constructor \u200b EvaluateResponse ( output , response ) Parameters: \u200b output : The output of the evaluated function. response : The response from the Aimon API analysis. Previous aimon.decorators.detect Next Low Level API Reference Class: Dataset aimon_client.datasets.create Example: Class: DatasetCollection aimon_client.datasets.collection.create Example: Function: evaluate Signature Parameters: Returns: Raises: Note: Example: Class: EvaluateResponse Constructor Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Evaluate | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Python API Reference Typescript API Reference Methods Detect Evaluate Overview Low Level API Example Applications SDKs Typescript API Reference Methods Evaluate On this page Evaluate Dataset \u200b A dataset is a collection of records that can be used for evaluations. The dataset should be a CSV file. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM \""metadata\"": This is a dictionary of additional metadata associated with the record. Create dataset: \u200b // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; const dataset1 = await createDataset ( \""/path/to/file/filename_1.csv\"" , \""filename1.csv\"" , \""description\"" ) ; const dataset2 = await createDataset ( \""/path/to/file/filename_2.csv\"" , \""filename2.csv\"" , \""description\"" ) ; DatasetCollection \u200b A dataset collection is a collection of one or more datasets that can be used for evaluations. aimon.datasets.collection.create \u200b This function creates a new dataset collection. Args: \u200b name ( str ): The name of the dataset collection. description ( str ): The description of the dataset collection. dataset_ids ( List[str] ): A list of dataset IDs to include in the collection. Returns: \u200b CollectionCreateResponse : The created dataset collection. Example: \u200b let datasetCollection : Client . Datasets . CollectionCreateResponse | undefined ; // Ensures that dataset1.sha and dataset2.sha are defined if ( dataset1 . sha && dataset2 . sha ) { // Creates dataset collection datasetCollection = await aimon . datasets . collection . create ( { name : \""my_first_dataset_collection\"" , dataset_ids : [ dataset1 . sha , dataset2 . sha ] , description : \""This is a collection of two datasets.\"" , } ) ; } else { throw new Error ( \""Dataset sha is undefined\"" ) ; } Function: evaluate \u200b Run an evaluation on a dataset collection using the Aimon API. Parameters: \u200b applicationName (str): The name of the application to run the evaluation on. modelName (str): The name of the model to run the evaluation on. datasetCollectionName (str): The name of the dataset collection to run the evaluation on. evaluationName (str): The name of the evaluation to be created. headers (list): A list of column names in the dataset to be used for the evaluation. Must include 'context_docs'. config (dict, optional): A dictionary of configuration options for the evaluation. Returns: \u200b List[EvaluateResponse]: A list of EvaluateResponse objects containing the output and response for each record in the dataset collection. Raises: \u200b ValueError: If headers is empty or doesn't contain 'context_docs', or if required fields are missing from the dataset records. Note: \u200b The dataset records must contain 'context_docs' and all fields specified in the 'headers' argument. The 'prompt', 'output', and 'instructions' fields are optional. Example: \u200b import Client from \""aimon\"" ; const aimon = new Client ( { authHeader : \""Bearer API_KEY\"" } ) ; headers = [ \""context_docs\"" , \""user_query\"" , \""output\"" ] ; config = { hallucination : { detector_name : \""default\"" } , instruction_adherence : { detector_name : \""default\"" } , } ; results = aimon . evaluate ( ( applicationName = \""my_app\"" ) , ( modelName = \""gpt-4o\"" ) , //this dataset collection must exist in the Aimon platform ( datasetCollectionName = \""my_dataset_collection\"" ) , ( evaluationName = \""my_evaluation\"" ) , ( headers = headers ) , ( config = config ) ) ; Previous Detect Next Overview Dataset Create dataset: DatasetCollection aimon.datasets.collection.create Example: Function: evaluate Parameters: Returns: Raises: Note: Example: Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""AnalyzeCreateResponse = await aimon . analyze . create ( [ { application_id : application . id , version : application . version , prompt : prompt !== null ? record . prompt : \""\"" , user_query : userQuery !== null ? record . userQuery : \""\"" , context_docs : record . contextDocs , output : record . output , evaluation_id : evaluationRun . evaluation_id , evaluation_run_id : evaluationRun . id , } , ] ) ; } Lower-level API \u200b If you need more control over the evaluation or continuous monitoring process, you can use the lower-level API described in this notebook . Glossary \u200b Evaluation \u200b Before deploying an LLM application to production, it is a good idea to test it with either a curated golden dataset or a snapshot of production traffic. AIMon platform provides detectors to assess the quality of the generated text in your dataset. AIMon adopts a \""batteries included\"" approach, i.e., you do not have to use another third-party API. Model \u200b A model is a generative model, typically an LLM, that generates text based on an input query, context and user provided instructions. The model can be a vanilla model, a fine-tuned model or a prompt-engineered model. When evaluating on a dataset, you simply tag your evaluation with a model name. Application \u200b An application is a specific use-case or a task that is associated with a model. For example, a summarization application. Each application is versioned i.e., each application is associated with a particular model for a given version of the application. When you use a different model for the same application, AIMon will automatically create a new version of the application. Previous Continuous Monitoring Next SDKs Uploading the Evaluation Dataset Combining Datasets into a Dataset Collection Running an Evaluation Lower-level API Glossary Copyright \u00a9 2024 Aimon Labs, Inc. All rights reserved"", ""Offline Evaluation | AIMon Skip to main content GenAI Reliability Community GitHub Overview Quick Start Glossary Detectors Continuous Monitoring Offline Evaluation SDKs Example Applications Offline Evaluation On this page Offline Evaluation This page explains how to quickly and easily evaluate your dataset of LLM prompts, contexts and responses using AIMon detectors (hallucination detector, toxicity detector, and others). Uploading the Evaluation Dataset \u200b Before evaluating on a dataset, you should create a dataset csv and upload it to the AIMon platform. A dataset is a CSV file that contains one or more of the supported columns listed below. A dataset is immutable once created. The supported columns are: \""prompt\"": This is the system prompt used for the LLM \""user_query\"": This the query specified by the user \""context_docs\"": These are context documents that are either retrieved from a RAG or through other methods. For tasks like summarization, these documents could be directly specified by the user. \""output\"": This is the generated text by the LLM \""instructions\"": These are the instructions provided to the LLM Depending on the detector being used, you may not need all the columns. For example, the hallucination detector only requires the \""context_docs\"" and \""output\"" columns. For the \""context_classification\"" detector, you would need the \""context_docs\"" column. The dataset creation API is designed to fail fast to provide you immediate feedback on the required columns for the detector you are using. Here is an example dataset evaluation_dataset.csv : prompt,user_query,context_docs,instructions,output \""Please provide information on the latest version of the Acme python client, including its features and release date.\"",\""\""\""What is the latest version of Acme python client?\""\""\"",\""[\""\""KB_Article_1: Acme supports Python, Javascript, and Java. The latest version of the python library is v2.1, which was launched in March 2024\""\"", \""\""KB_Article_2: Acme has deep integrations with the Python ecosystem where the Python client has shown to add value to developers\""\"", \""\""KB_Article_3: The Acme python client version 2.1 introduces new features like async support and improved error handling.\""\""]\"",\""1. Ensure that the response in under 500 words, 2. Ensure that there is no mention of the word \""\""Typescript\""\""\"",The latest version is 2.1 and has async support. It was launched in March 2024 Could you explain how to configure the Acme python client for a new project?,\""\""\""How do I configure the Acme python client?\""\""\"",\""[\""\""KB_Article_4: Configuring the Acme python client involves setting up the environment variables first, followed by installing the necessary dependencies.\""\"", \""\""KB_Article_5: Detailed configuration steps for the Acme client can be found in the official documentation. It covers both basic and advanced setups.\""\""]\"",\""1. Ensure that the response in under 500 words 2. Ensure that there is no mention of the word \""\""Typescript\""\"" 3. Ensure the response is in english\"",\"" Setup the environment variables, install dependencies and follow the official documentation for configuration\""\""\"" Upload the dataset using the AIMon client as follows: Python TypeScript from aimon import Client import json aimon_client = Client ( auth_header = \""Bearer <AIMON API KEY>\"" ) # Create a new dataset file_path = \""evaluation_dataset.csv\"" dataset_args = json . dumps ( { \""name\"" : \""evaluation_dataset.csv\"" , \""description\"" : \""This is a golden dataset\"" } ) with open ( file_path , 'rb' ) as file1 : aimon_dataset = aimon_client . datasets . create ( file = file1 , json_data = dataset_args ) import Client from \""aimon\"" ; const aimon_client = new Client ( { authHeader : ` Bearer API_KEY ` , } ) ; // Creates a new dataset from the local path csv file const createDataset = async ( path : string , datasetName : string , description : string ) : Promise < Client . Dataset > => { const file = await fileFromPath ( path ) ; const json_data = JSON . stringify ( { name : datasetName , description : description , } ) ; const params = { file : file , json_data : json_data , } ; const dataset : Client . Dataset = await aimon . datasets . create ( params ) ; return dataset ; } ; Combining Datasets into a Dataset Collection \u200b Group evaluation datasets into a collection for ease of use: Python TypeScript dataset_collection = aimon_client . datasets . collection . create ( name = \""my_first_dataset_collection\"" , dataset_ids = [ aimon_dataset . sha , ] , description = \""This is a collection containing just one dataset.\""""]",Are AIMon detectors capable of sensing humour in user's questions? What about irony. Are they able to detect it as well?,1. Discuss the technology behind detection capabilities. 2. Provide examples of how humor and irony might be identified.,Investigate AIMon's capabilities in detecting subtle nuances like humor and irony in user inquiries.,"The capabilities of AIMon detectors primarily focus on evaluating the quality of generated text, such as detecting hallucinations or adherence to instructions. There is no specific mention of the ability to detect humor or irony in user inquiries within the provided information. Therefore, it is unclear whether AIMon detectors can effectively sense these subtle nuances."
